{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0177a5bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\super\\DAG\\nodag\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "import numpy as np\n",
    "from numpy.linalg import LinAlgError, inv\n",
    "from scipy.linalg import sqrtm\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"..\")))\n",
    "sys.path.append(r\"C:\\Users\\super\\DAG\")\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "672a65fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nodag.nodag_gs_findbest import nodag_findbest_loss, nodag_findbest_likelihood_penalty, calculate_shd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d5ad621c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random seed =  0\n",
      "t =  0\n",
      "t =  100\n",
      "t =  200\n",
      "shd =  0\n",
      "likelihood_true =  3.9726462866418357\n",
      "G_true = \n",
      " [[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n",
      "G_est = \n",
      " [[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n",
      "Final Loss =  4.001982481771426\n",
      "Final penalty =  0.030033679002612855\n",
      "Final likelihood =  3.9719488027688135\n",
      "seed =  223\n",
      "\n",
      "random seed =  1\n",
      "t =  0\n",
      "t =  100\n",
      "t =  200\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 37\u001b[0m\n\u001b[0;32m     33\u001b[0m likelihood_true \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mlog(np\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mdet(A_true)) \u001b[38;5;241m+\u001b[39m np\u001b[38;5;241m.\u001b[39mtrace(A_true\u001b[38;5;241m.\u001b[39mT \u001b[38;5;241m@\u001b[39m R_hat \u001b[38;5;241m@\u001b[39m A_true)\n\u001b[0;32m     36\u001b[0m best_G, best_B, best_loss, best_likelihood, best_penalty, best_seed \u001b[38;5;241m=\u001b[39m nodag_findbest_likelihood_penalty(R_hat \u001b[38;5;241m=\u001b[39m R_hat,lam \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.01\u001b[39m, times \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m300\u001b[39m)\n\u001b[1;32m---> 37\u001b[0m shd, est_cpdag, true_cpdag \u001b[38;5;241m=\u001b[39m calculate_shd(best_G, G_true)\n\u001b[0;32m     38\u001b[0m shd_sum \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m shd\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshd = \u001b[39m\u001b[38;5;124m\"\u001b[39m,shd)\n",
      "File \u001b[1;32mc:\\Users\\super\\DAG\\nodag\\nodag_gs_findbest.py:116\u001b[0m, in \u001b[0;36mcalculate_shd\u001b[1;34m(est, true)\u001b[0m\n\u001b[0;32m    104\u001b[0m candidates \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    105\u001b[0m     r \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m results\n\u001b[0;32m    106\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mabs\u001b[39m(r[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlikelihood\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m-\u001b[39m min_likelihood) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m eps\n\u001b[0;32m    107\u001b[0m ]\n\u001b[0;32m    109\u001b[0m best \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(candidates, key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m r: r[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpenalty\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m    112\u001b[0m     best[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mG\u001b[39m\u001b[38;5;124m\"\u001b[39m], best[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mB\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    113\u001b[0m     best[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mP\u001b[39m\u001b[38;5;124m\"\u001b[39m], best[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mU\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    114\u001b[0m     best[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m], best[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlikelihood\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    115\u001b[0m     best[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpenalty\u001b[39m\u001b[38;5;124m\"\u001b[39m], best[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseed\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m--> 116\u001b[0m )\n",
      "File \u001b[1;32md:\\coding\\Anaconda\\Lib\\site-packages\\causallearn\\utils\\DAG2CPDAG.py:28\u001b[0m, in \u001b[0;36mdag2cpdag\u001b[1;34m(G)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;124;03mConvert a DAG to its corresponding PDAG\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;124;03mYuequn Liu@dmirlab, Wei Chen@dmirlab, Kun Zhang@CMU\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# order the edges in G\u001b[39;00m\n\u001b[0;32m     27\u001b[0m nodes_order \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\n\u001b[1;32m---> 28\u001b[0m     \u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28;01mlambda\u001b[39;00m x: G\u001b[38;5;241m.\u001b[39mnode_map[x], G\u001b[38;5;241m.\u001b[39mget_causal_ordering())\n\u001b[0;32m     29\u001b[0m )  \u001b[38;5;66;03m# Perform a topological sort on the nodes of G\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# nodes_order(1) is the node which has the highest order\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# nodes_order(N) is the node which has the lowest order\u001b[39;00m\n\u001b[0;32m     32\u001b[0m edges_order \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mempty((\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m), dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mint64)\n",
      "File \u001b[1;32md:\\coding\\Anaconda\\Lib\\site-packages\\causallearn\\graph\\Dag.py:721\u001b[0m, in \u001b[0;36mDag.get_causal_ordering\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    719\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_causal_ordering\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    720\u001b[0m     utils \u001b[38;5;241m=\u001b[39m GraphUtils()\n\u001b[1;32m--> 721\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m utils\u001b[38;5;241m.\u001b[39mget_causal_order(\u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[1;32md:\\coding\\Anaconda\\Lib\\site-packages\\causallearn\\utils\\GraphUtils.py:279\u001b[0m, in \u001b[0;36mGraphUtils.get_causal_order\u001b[1;34m(self, graph)\u001b[0m\n\u001b[0;32m    276\u001b[0m sub_not_found: List[Node] \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    277\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m node \u001b[38;5;129;01min\u001b[39;00m not_found:\n\u001b[0;32m    278\u001b[0m     \u001b[38;5;66;03m# print(node)\u001b[39;00m\n\u001b[1;32m--> 279\u001b[0m     parents \u001b[38;5;241m=\u001b[39m graph\u001b[38;5;241m.\u001b[39mget_parents(node)\n\u001b[0;32m    280\u001b[0m     sub_parents: List[Node] \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    281\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m node1 \u001b[38;5;129;01min\u001b[39;00m parents:\n",
      "File \u001b[1;32md:\\coding\\Anaconda\\Lib\\site-packages\\causallearn\\graph\\Dag.py:231\u001b[0m, in \u001b[0;36mDag.get_parents\u001b[1;34m(self, node)\u001b[0m\n\u001b[0;32m    228\u001b[0m parents \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    230\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnodes)):\n\u001b[1;32m--> 231\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgraph[i, j] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    232\u001b[0m         node2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnodes[j]\n\u001b[0;32m    233\u001b[0m         parents\u001b[38;5;241m.\u001b[39mappend(node2)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Find best penalty in best likelihoods, seed0-9\n",
    "# d = 4, lam = 0.01\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from synthetic_dataset import SyntheticDataset\n",
    "from synthetic_dataset import dataset_based_on_B\n",
    "\n",
    "shd_sum = 0\n",
    "times = 10\n",
    "\n",
    "for seed in range(times):\n",
    "    # Load dataset\n",
    "    n, d = 10000, 4\n",
    "    graph_type, degree = 'ER', 1    # ER1 graph\n",
    "    B_scale = 1.0\n",
    "    noise_type = 'gaussian_ev'\n",
    "    print(\"random seed = \", seed)\n",
    "\n",
    "    Dataset = SyntheticDataset(n, d, graph_type, degree,\n",
    "                            noise_type, B_scale, seed=seed)\n",
    "    data = Dataset.X\n",
    "    B_true = Dataset.B\n",
    "    G_true = Dataset.B_bin\n",
    "\n",
    "    R_hat = np.cov(data.T)\n",
    "    d = R_hat.shape[0]\n",
    "    Omega_true = np.eye(d)\n",
    "    Theta_hat = inv(R_hat)\n",
    "\n",
    "    A_true = np.eye(d) - B_true\n",
    "    # print(\"A_true = \\n\",A_true)\n",
    "    likelihood_true = - 2 * np.log(np.linalg.det(A_true)) + np.trace(A_true.T @ R_hat @ A_true)\n",
    "\n",
    "\n",
    "    best_G, best_B, best_loss, best_likelihood, best_penalty, best_seed = nodag_findbest_likelihood_penalty(R_hat = R_hat,lam = 0.01, times = 300)\n",
    "    shd, est_cpdag, true_cpdag = calculate_shd(best_G, G_true)\n",
    "    shd_sum += shd\n",
    "\n",
    "    print(\"shd = \",shd)\n",
    "    print(\"likelihood_true = \", likelihood_true)\n",
    "    print(\"G_true = \\n\", G_true)\n",
    "    print(\"G_est = \\n\", best_G)\n",
    "    # print(\"Is in MEC: \", MEC.is_in_markov_equiv_class(G_true, best_B))\n",
    "    print(\"Final Loss = \", best_loss)\n",
    "    print(\"Final penalty = \", best_penalty)\n",
    "    print(\"Final likelihood = \", best_likelihood)\n",
    "    print(\"seed = \", best_seed)\n",
    "    print(\"\")\n",
    "print(\"\\n Average shd = \", shd_sum / times)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6d9a98",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc78ed3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find best penalty in best likelihoods, seed0-9\n",
    "# d = 4, lam = 0.1\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from synthetic_dataset import SyntheticDataset\n",
    "from synthetic_dataset import dataset_based_on_B\n",
    "\n",
    "shd_sum = 0\n",
    "times = 10\n",
    "\n",
    "for seed in range(times):\n",
    "    # Load dataset\n",
    "    n, d = 10000, 4\n",
    "    graph_type, degree = 'ER', 1    # ER1 graph\n",
    "    B_scale = 1.0\n",
    "    noise_type = 'gaussian_ev'\n",
    "    print(\"random seed = \", seed)\n",
    "\n",
    "    Dataset = SyntheticDataset(n, d, graph_type, degree,\n",
    "                            noise_type, B_scale, seed=seed)\n",
    "    data = Dataset.X\n",
    "    B_true = Dataset.B\n",
    "    G_true = Dataset.B_bin\n",
    "\n",
    "    R_hat = np.cov(data.T)\n",
    "    d = R_hat.shape[0]\n",
    "    Omega_true = np.eye(d)\n",
    "    Theta_hat = inv(R_hat)\n",
    "\n",
    "    A_true = np.eye(d) - B_true\n",
    "    # print(\"A_true = \\n\",A_true)\n",
    "    likelihood_true = - 2 * np.log(np.linalg.det(A_true)) + np.trace(A_true.T @ R_hat @ A_true)\n",
    "\n",
    "\n",
    "    best_G, best_B, best_loss, best_likelihood, best_penalty, best_seed = nodag_findbest_likelihood_penalty(R_hat = R_hat,lam = 0.1, times = 300)\n",
    "    shd, est_cpdag, true_cpdag = calculate_shd(best_G, G_true)\n",
    "    shd_sum += shd\n",
    "\n",
    "    print(\"shd = \",shd)\n",
    "    print(\"likelihood_true = \", likelihood_true)\n",
    "    print(\"G_true = \\n\", G_true)\n",
    "    print(\"G_est = \\n\", best_G)\n",
    "    # print(\"Is in MEC: \", MEC.is_in_markov_equiv_class(G_true, best_B))\n",
    "    print(\"Final Loss = \", best_loss)\n",
    "    print(\"Final penalty = \", best_penalty)\n",
    "    print(\"Final likelihood = \", best_likelihood)\n",
    "    print(\"seed = \", best_seed)\n",
    "    print(\"\")\n",
    "print(\"\\n Average shd = \", shd_sum / times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0c9f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find best penalty in best likelihoods, seed0-9\n",
    "# d = 4, lam = 1\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from synthetic_dataset import SyntheticDataset\n",
    "from synthetic_dataset import dataset_based_on_B\n",
    "\n",
    "shd_sum = 0\n",
    "times = 10\n",
    "\n",
    "for seed in range(times):\n",
    "    # Load dataset\n",
    "    n, d = 10000, 4\n",
    "    graph_type, degree = 'ER', 1    # ER1 graph\n",
    "    B_scale = 1.0\n",
    "    noise_type = 'gaussian_ev'\n",
    "    print(\"random seed = \", seed)\n",
    "\n",
    "    Dataset = SyntheticDataset(n, d, graph_type, degree,\n",
    "                            noise_type, B_scale, seed=seed)\n",
    "    data = Dataset.X\n",
    "    B_true = Dataset.B\n",
    "    G_true = Dataset.B_bin\n",
    "\n",
    "    R_hat = np.cov(data.T)\n",
    "    d = R_hat.shape[0]\n",
    "    Omega_true = np.eye(d)\n",
    "    Theta_hat = inv(R_hat)\n",
    "\n",
    "    A_true = np.eye(d) - B_true\n",
    "    # print(\"A_true = \\n\",A_true)\n",
    "    likelihood_true = - 2 * np.log(np.linalg.det(A_true)) + np.trace(A_true.T @ R_hat @ A_true)\n",
    "\n",
    "\n",
    "    best_G, best_B, best_loss, best_likelihood, best_penalty, best_seed = nodag_findbest_likelihood_penalty(R_hat = R_hat,lam = 1, times = 300)\n",
    "    shd, est_cpdag, true_cpdag = calculate_shd(best_G, G_true)\n",
    "    shd_sum += shd\n",
    "\n",
    "    print(\"shd = \",shd)\n",
    "    print(\"likelihood_true = \", likelihood_true)\n",
    "    print(\"G_true = \\n\", G_true)\n",
    "    print(\"G_est = \\n\", best_G)\n",
    "    # print(\"Is in MEC: \", MEC.is_in_markov_equiv_class(G_true, best_B))\n",
    "    print(\"Final Loss = \", best_loss)\n",
    "    print(\"Final penalty = \", best_penalty)\n",
    "    print(\"Final likelihood = \", best_likelihood)\n",
    "    print(\"seed = \", best_seed)\n",
    "    print(\"\")\n",
    "print(\"\\n Average shd = \", shd_sum / times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc3b71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find best penalty in best likelihoods, seed0-9\n",
    "# d = 4, lam = 10\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from synthetic_dataset import SyntheticDataset\n",
    "from synthetic_dataset import dataset_based_on_B\n",
    "\n",
    "shd_sum = 0\n",
    "times = 10\n",
    "\n",
    "for seed in range(times):\n",
    "    # Load dataset\n",
    "    n, d = 10000, 4\n",
    "    graph_type, degree = 'ER', 1    # ER1 graph\n",
    "    B_scale = 1.0\n",
    "    noise_type = 'gaussian_ev'\n",
    "    print(\"random seed = \", seed)\n",
    "\n",
    "    Dataset = SyntheticDataset(n, d, graph_type, degree,\n",
    "                            noise_type, B_scale, seed=seed)\n",
    "    data = Dataset.X\n",
    "    B_true = Dataset.B\n",
    "    G_true = Dataset.B_bin\n",
    "\n",
    "    R_hat = np.cov(data.T)\n",
    "    d = R_hat.shape[0]\n",
    "    Omega_true = np.eye(d)\n",
    "    Theta_hat = inv(R_hat)\n",
    "\n",
    "    A_true = np.eye(d) - B_true\n",
    "    # print(\"A_true = \\n\",A_true)\n",
    "    likelihood_true = - 2 * np.log(np.linalg.det(A_true)) + np.trace(A_true.T @ R_hat @ A_true)\n",
    "\n",
    "\n",
    "    best_G, best_B, best_loss, best_likelihood, best_penalty, best_seed = nodag_findbest_likelihood_penalty(R_hat = R_hat,lam = 10, times = 300)\n",
    "    shd, est_cpdag, true_cpdag = calculate_shd(best_G, G_true)\n",
    "    shd_sum += shd\n",
    "\n",
    "    print(\"shd = \",shd)\n",
    "    print(\"likelihood_true = \", likelihood_true)\n",
    "    print(\"G_true = \\n\", G_true)\n",
    "    print(\"G_est = \\n\", best_G)\n",
    "    # print(\"Is in MEC: \", MEC.is_in_markov_equiv_class(G_true, best_B))\n",
    "    print(\"Final Loss = \", best_loss)\n",
    "    print(\"Final penalty = \", best_penalty)\n",
    "    print(\"Final likelihood = \", best_likelihood)\n",
    "    print(\"seed = \", best_seed)\n",
    "    print(\"\")\n",
    "print(\"\\n Average shd = \", shd_sum / times)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
